{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LEAD or LAG\n",
    "\n",
    "Let us understand the usage of `LEAD` or `LAG` functions. Both are used for similar scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Windowing Functions'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = \"/public/airtraffic_all/airtraffic-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark. \\\n",
    "  read. \\\n",
    "  parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, lpad, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window. \\\n",
    "    partitionBy(\"FlightDate\", \"Origin\"). \\\n",
    "    orderBy(col(\"CRSDepTime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic. \\\n",
    "    filter(\"IsDepDelayed = 'YES' and Cancelled = 0\"). \\\n",
    "    select(concat(\"Year\", \n",
    "                  lpad(\"Month\", 2, \"0\"), \n",
    "                  lpad(\"DayOfMonth\", 2, \"0\")\n",
    "                 ).alias(\"FlightDate\"),\n",
    "           \"Origin\",\n",
    "           \"UniqueCarrier\",\n",
    "           \"FlightNum\",\n",
    "           \"CRSDepTime\",\n",
    "           \"IsDepDelayed\",\n",
    "           col(\"DepDelay\").cast(\"int\").alias(\"DepDelay\")\n",
    "          ). \\\n",
    "    withColumn(\"LeadUniqueCarrier\", lead(\"UniqueCarrier\").over(spec)). \\\n",
    "    withColumn(\"LeadFlightNum\", lead(\"FlightNum\").over(spec)). \\\n",
    "    withColumn(\"LeadCRSDepTime\", lead(\"CRSDepTime\").over(spec)). \\\n",
    "    withColumn(\"LeadDepDelay\", lead(\"DepDelay\").over(spec)). \\\n",
    "    orderBy(\"FlightDate\", \"Origin\", \"CRSDepTime\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LEAD or LAG with 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = \"/public/airtraffic_all/airtraffic-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark. \\\n",
    "    read. \\\n",
    "    parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, lpad, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, lead, substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window. \\\n",
    "    partitionBy(substring(\"FlightDate\", 1, 6), \"Origin\"). \\\n",
    "    orderBy(\"FlightDate\", col(\"TotalDepDelay\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic. \\\n",
    "    filter(\"\"\"IsDepDelayed = 'YES' \n",
    "              AND Cancelled = 0\n",
    "              AND concat(Year, \n",
    "                         lpad(Month, 2, '0'),\n",
    "                         lpad(DayOfMonth, 2, '0')\n",
    "                        ) BETWEEN 20080101 AND 20080114\n",
    "              AND Origin IN ('ATL', 'DFW', 'JFK', 'LAX', 'SFO', 'ORD')\n",
    "           \"\"\"\n",
    "          ). \\\n",
    "    groupBy(concat(\"Year\", \n",
    "                   lpad(\"Month\", 2, \"0\"), \n",
    "                   lpad(\"DayOfMonth\", 2, \"0\")\n",
    "                  ).alias(\"FlightDate\"), \n",
    "            \"Origin\"\n",
    "           ). \\\n",
    "    agg(sum(col(\"DepDelay\").cast(\"int\")).alias(\"TotalDepDelay\")). \\\n",
    "    withColumn(\"LeadFlightDate\", lead(\"FlightDate\", 7).over(spec)). \\\n",
    "    withColumn(\"LeadOrigin\", lead(\"Origin\", 7).over(spec)). \\\n",
    "    withColumn(\"LeadTotalDepDelay\", lead(\"TotalDepDelay\", 7).over(spec)). \\\n",
    "    filter('Origin = \"ORD\"'). \\\n",
    "    orderBy(\"FlightDate\", col(\"TotalDepDelay\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic. \\\n",
    "    filter(\"\"\"IsDepDelayed = 'YES' \n",
    "              AND Cancelled = 0\n",
    "              AND concat(Year, \n",
    "                         lpad(Month, 2, '0'),\n",
    "                         lpad(DayOfMonth, 2, '0')\n",
    "                        ) BETWEEN 20080101 AND 20080114\n",
    "              AND Origin IN ('ATL', 'DFW', 'JFK', 'LAX', 'SFO', 'ORD')\n",
    "           \"\"\"\n",
    "          ). \\\n",
    "    groupBy(concat(\"Year\", \n",
    "                   lpad(\"Month\", 2, \"0\"), \n",
    "                   lpad(\"DayOfMonth\", 2, \"0\")\n",
    "                  ).alias(\"FlightDate\"), \n",
    "            \"Origin\"\n",
    "           ). \\\n",
    "    agg(sum(col(\"DepDelay\").cast(\"int\")).alias(\"TotalDepDelay\")). \\\n",
    "    withColumn(\"LeadFlightDate\", lead(\"FlightDate\", 7).over(spec)). \\\n",
    "    withColumn(\"LeadOrigin\", lead(\"Origin\", 7).over(spec)). \\\n",
    "    withColumn(\"LeadTotalDepDelay\", lead(\"TotalDepDelay\", 7).over(spec)). \\\n",
    "    filter('Origin = \"ORD\" AND FlightDate BETWEEN 20080101 AND 20080107'). \\\n",
    "    orderBy(\"FlightDate\", col(\"TotalDepDelay\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic. \\\n",
    "    filter(\"\"\"IsDepDelayed = 'YES' \n",
    "              AND Cancelled = 0\n",
    "              AND concat(Year, \n",
    "                         lpad(Month, 2, '0'),\n",
    "                         lpad(DayOfMonth, 2, '0')\n",
    "                        ) BETWEEN 20080101 AND 20080114\n",
    "              AND Origin IN ('ATL', 'DFW', 'JFK', 'LAX', 'SFO', 'ORD')\n",
    "           \"\"\"\n",
    "          ). \\\n",
    "    groupBy(concat(\"Year\", \n",
    "                   lpad(\"Month\", 2, \"0\"), \n",
    "                   lpad(\"DayOfMonth\", 2, \"0\")\n",
    "                  ).alias(\"FlightDate\"), \n",
    "            \"Origin\"\n",
    "           ). \\\n",
    "    agg(sum(col(\"DepDelay\").cast(\"int\")).alias(\"TotalDepDelay\")). \\\n",
    "    withColumn(\"LeadFlightDate\", lead(\"FlightDate\", 7).over(spec)). \\\n",
    "    withColumn(\"LeadOrigin\", lead(\"Origin\", 7).over(spec)). \\\n",
    "    withColumn(\"LeadTotalDepDelay\", lead(\"TotalDepDelay\", 7).over(spec)). \\\n",
    "    filter('FlightDate BETWEEN 20080101 AND 20080107'). \\\n",
    "    orderBy(\"FlightDate\", col(\"TotalDepDelay\").desc()). \\\n",
    "    show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
