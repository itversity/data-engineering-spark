{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Functions Examples\n",
    "\n",
    "Let us perform few tasks to understand the usage of aggregate functions.\n",
    "* Get all the employees details who are making more than average department salary expense.\n",
    "* Get all the employees details who are making less than 50% of the highest salaries employee in each department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Windowing Functions'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * First let us understand relevance of these functions using `employees` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesPath = '/public/hr_db/employees'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = spark. \\\n",
    "    read. \\\n",
    "    format('csv'). \\\n",
    "    option('sep', '\\t'). \\\n",
    "    schema('''employee_id INT, \n",
    "              first_name STRING, \n",
    "              last_name STRING, \n",
    "              email STRING,\n",
    "              phone_number STRING, \n",
    "              hire_date STRING, \n",
    "              job_id STRING, \n",
    "              salary FLOAT,\n",
    "              commission_pct STRING,\n",
    "              manager_id STRING, \n",
    "              department_id STRING\n",
    "            '''). \\\n",
    "    load(employeesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------+------------------+----------+--------+-------+--------------+----------+-------------+\n",
      "|employee_id|first_name| last_name|   email|      phone_number| hire_date|  job_id| salary|commission_pct|manager_id|department_id|\n",
      "+-----------+----------+----------+--------+------------------+----------+--------+-------+--------------+----------+-------------+\n",
      "|        127|     James|    Landry| JLANDRY|      650.124.1334|1999-01-14|ST_CLERK| 2400.0|          null|       120|           50|\n",
      "|        128|    Steven|    Markle| SMARKLE|      650.124.1434|2000-03-08|ST_CLERK| 2200.0|          null|       120|           50|\n",
      "|        129|     Laura|    Bissot| LBISSOT|      650.124.5234|1997-08-20|ST_CLERK| 3300.0|          null|       121|           50|\n",
      "|        130|     Mozhe|  Atkinson|MATKINSO|      650.124.6234|1997-10-30|ST_CLERK| 2800.0|          null|       121|           50|\n",
      "|        131|     James|    Marlow| JAMRLOW|      650.124.7234|1997-02-16|ST_CLERK| 2500.0|          null|       121|           50|\n",
      "|        132|        TJ|     Olson| TJOLSON|      650.124.8234|1999-04-10|ST_CLERK| 2100.0|          null|       121|           50|\n",
      "|        133|     Jason|    Mallin| JMALLIN|      650.127.1934|1996-06-14|ST_CLERK| 3300.0|          null|       122|           50|\n",
      "|        134|   Michael|    Rogers| MROGERS|      650.127.1834|1998-08-26|ST_CLERK| 2900.0|          null|       122|           50|\n",
      "|        135|        Ki|       Gee|    KGEE|      650.127.1734|1999-12-12|ST_CLERK| 2400.0|          null|       122|           50|\n",
      "|        136|     Hazel|Philtanker|HPHILTAN|      650.127.1634|2000-02-06|ST_CLERK| 2200.0|          null|       122|           50|\n",
      "|        137|    Renske|    Ladwig| RLADWIG|      650.121.1234|1995-07-14|ST_CLERK| 3600.0|          null|       123|           50|\n",
      "|        138|   Stephen|    Stiles| SSTILES|      650.121.2034|1997-10-26|ST_CLERK| 3200.0|          null|       123|           50|\n",
      "|        139|      John|       Seo|    JSEO|      650.121.2019|1998-02-12|ST_CLERK| 2700.0|          null|       123|           50|\n",
      "|        140|    Joshua|     Patel|  JPATEL|      650.121.1834|1998-04-06|ST_CLERK| 2500.0|          null|       123|           50|\n",
      "|        141|    Trenna|      Rajs|   TRAJS|      650.121.8009|1995-10-17|ST_CLERK| 3500.0|          null|       124|           50|\n",
      "|        142|    Curtis|    Davies| CDAVIES|      650.121.2994|1997-01-29|ST_CLERK| 3100.0|          null|       124|           50|\n",
      "|        143|   Randall|     Matos|  RMATOS|      650.121.2874|1998-03-15|ST_CLERK| 2600.0|          null|       124|           50|\n",
      "|        144|     Peter|    Vargas| PVARGAS|      650.121.2004|1998-07-09|ST_CLERK| 2500.0|          null|       124|           50|\n",
      "|        145|      John|   Russell| JRUSSEL|011.44.1344.429268|1996-10-01|  SA_MAN|14000.0|          0.40|       100|           80|\n",
      "|        146|     Karen|  Partners|KPARTNER|011.44.1344.467268|1997-01-05|  SA_MAN|13500.0|          0.30|       100|           80|\n",
      "+-----------+----------+----------+--------+------------------+----------+--------+-------+--------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- job_id: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- commission_pct: string (nullable = true)\n",
      " |-- manager_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+\n",
      "|employee_id|department_id| salary|\n",
      "+-----------+-------------+-------+\n",
      "|        178|         null| 7000.0|\n",
      "|        200|           10| 4400.0|\n",
      "|        202|           20| 6000.0|\n",
      "|        201|           20|13000.0|\n",
      "|        119|           30| 2500.0|\n",
      "|        118|           30| 2600.0|\n",
      "|        117|           30| 2800.0|\n",
      "|        116|           30| 2900.0|\n",
      "|        115|           30| 3100.0|\n",
      "|        114|           30|11000.0|\n",
      "|        203|           40| 6500.0|\n",
      "|        132|           50| 2100.0|\n",
      "|        128|           50| 2200.0|\n",
      "|        136|           50| 2200.0|\n",
      "|        135|           50| 2400.0|\n",
      "|        127|           50| 2400.0|\n",
      "|        131|           50| 2500.0|\n",
      "|        140|           50| 2500.0|\n",
      "|        191|           50| 2500.0|\n",
      "|        144|           50| 2500.0|\n",
      "+-----------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees. \\\n",
    "    select('employee_id', \n",
    "           col('department_id').cast('int').alias('department_id'), \n",
    "           'salary'\n",
    "          ). \\\n",
    "    orderBy('department_id', 'salary'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us say we want to compare individual salary with department wise salary expense.\n",
    "* Here is one of the approach which require self join.\n",
    "  * Compute department wise expense usig `groupBy` and `agg`.\n",
    "  * Join with **employees** again on department_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_expense = employees. \\\n",
    "    groupBy('department_id'). \\\n",
    "    agg(sum('salary').alias('expense'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|department_id| expense|\n",
      "+-------------+--------+\n",
      "|           30| 24900.0|\n",
      "|          110| 20300.0|\n",
      "|          100| 51600.0|\n",
      "|           70| 10000.0|\n",
      "|           90| 58000.0|\n",
      "|           60| 28800.0|\n",
      "|           40|  6500.0|\n",
      "|           20| 19000.0|\n",
      "|           10|  4400.0|\n",
      "|           80|304500.0|\n",
      "|         null|  7000.0|\n",
      "|           50|156400.0|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_expense.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+-------------+--------+\n",
      "|employee_id|department_id| salary|department_id| expense|\n",
      "+-----------+-------------+-------+-------------+--------+\n",
      "|        200|           10| 4400.0|           10|  4400.0|\n",
      "|        113|          100| 6900.0|          100| 51600.0|\n",
      "|        111|          100| 7700.0|          100| 51600.0|\n",
      "|        112|          100| 7800.0|          100| 51600.0|\n",
      "|        110|          100| 8200.0|          100| 51600.0|\n",
      "|        109|          100| 9000.0|          100| 51600.0|\n",
      "|        108|          100|12000.0|          100| 51600.0|\n",
      "|        206|          110| 8300.0|          110| 20300.0|\n",
      "|        205|          110|12000.0|          110| 20300.0|\n",
      "|        202|           20| 6000.0|           20| 19000.0|\n",
      "|        201|           20|13000.0|           20| 19000.0|\n",
      "|        119|           30| 2500.0|           30| 24900.0|\n",
      "|        118|           30| 2600.0|           30| 24900.0|\n",
      "|        117|           30| 2800.0|           30| 24900.0|\n",
      "|        116|           30| 2900.0|           30| 24900.0|\n",
      "|        115|           30| 3100.0|           30| 24900.0|\n",
      "|        114|           30|11000.0|           30| 24900.0|\n",
      "|        203|           40| 6500.0|           40|  6500.0|\n",
      "|        132|           50| 2100.0|           50|156400.0|\n",
      "|        136|           50| 2200.0|           50|156400.0|\n",
      "+-----------+-------------+-------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees. \\\n",
    "    select('employee_id', 'department_id', 'salary'). \\\n",
    "    join(department_expense, employees.department_id == department_expense.department_id). \\\n",
    "    orderBy(employees.department_id, col('salary')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **However, using this approach is not very efficient and also overly complicated. Windowing functions actually simplify the logic and also runs efficiently**\n",
    " \n",
    "Now let us get into the details related to Windowing functions.\n",
    " * Main package `pyspark.sql.window`\n",
    " * It has classes such as `Window` and `WindowSpec`\n",
    " * `Window` have APIs such as `partitionBy`, `orderBy` etc\n",
    " * These APIs (such as `partitionBy`) return `WindowSpec` object. We can pass `WindowSpec` object to over on functions such as `rank()`, `dense_rank()`, `sum()` etc\n",
    " * Syntax: `sum().over(spec)` where `spec = Window.partitionBy('ColumnName')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.window in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.window\n",
      "\n",
      "DESCRIPTION\n",
      "    # Licensed to the Apache Software Foundation (ASF) under one or more\n",
      "    # contributor license agreements.  See the NOTICE file distributed with\n",
      "    # this work for additional information regarding copyright ownership.\n",
      "    # The ASF licenses this file to You under the Apache License, Version 2.0\n",
      "    # (the \"License\"); you may not use this file except in compliance with\n",
      "    # the License.  You may obtain a copy of the License at\n",
      "    #\n",
      "    #    http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "    #\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Window\n",
      "        WindowSpec\n",
      "    \n",
      "    class Window(builtins.object)\n",
      "     |  Utility functions for defining window in DataFrames.\n",
      "     |  \n",
      "     |  For example:\n",
      "     |  \n",
      "     |  >>> # ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
      "     |  >>> window = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      "     |  \n",
      "     |  >>> # PARTITION BY country ORDER BY date RANGE BETWEEN 3 PRECEDING AND 3 FOLLOWING\n",
      "     |  >>> window = Window.orderBy(\"date\").partitionBy(\"country\").rangeBetween(-3, 3)\n",
      "     |  \n",
      "     |  .. note:: When ordering is not defined, an unbounded window frame (rowFrame,\n",
      "     |       unboundedPreceding, unboundedFollowing) is used by default. When ordering is defined,\n",
      "     |       a growing window frame (rangeFrame, unboundedPreceding, currentRow) is used by default.\n",
      "     |  \n",
      "     |  .. note:: Experimental\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  orderBy(*cols)\n",
      "     |      Creates a :class:`WindowSpec` with the ordering defined.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  partitionBy(*cols)\n",
      "     |      Creates a :class:`WindowSpec` with the partitioning defined.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  rangeBetween(start, end)\n",
      "     |      Creates a :class:`WindowSpec` with the frame boundaries defined,\n",
      "     |      from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative from the current row. For example,\n",
      "     |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n",
      "     |      and \"5\" means the five off after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      :param start: boundary start, inclusive.\n",
      "     |                    The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |                    any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      "     |      :param end: boundary end, inclusive.\n",
      "     |                  The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |                  any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1\n",
      "     |  \n",
      "     |  rowsBetween(start, end)\n",
      "     |      Creates a :class:`WindowSpec` with the frame boundaries defined,\n",
      "     |      from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative positions from the current row.\n",
      "     |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n",
      "     |      the current row, and \"5\" means the fifth row after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      :param start: boundary start, inclusive.\n",
      "     |                    The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |                    any value less than or equal to -9223372036854775808.\n",
      "     |      :param end: boundary end, inclusive.\n",
      "     |                  The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |                  any value greater than or equal to 9223372036854775807.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  currentRow = 0\n",
      "     |  \n",
      "     |  unboundedFollowing = 9223372036854775807\n",
      "     |  \n",
      "     |  unboundedPreceding = -9223372036854775808\n",
      "    \n",
      "    class WindowSpec(builtins.object)\n",
      "     |  A window specification that defines the partitioning, ordering,\n",
      "     |  and frame boundaries.\n",
      "     |  \n",
      "     |  Use the static methods in :class:`Window` to create a :class:`WindowSpec`.\n",
      "     |  \n",
      "     |  .. note:: Experimental\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, jspec)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  orderBy(self, *cols)\n",
      "     |      Defines the ordering columns in a :class:`WindowSpec`.\n",
      "     |      \n",
      "     |      :param cols: names of columns or expressions\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  partitionBy(self, *cols)\n",
      "     |      Defines the partitioning columns in a :class:`WindowSpec`.\n",
      "     |      \n",
      "     |      :param cols: names of columns or expressions\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  rangeBetween(self, start, end)\n",
      "     |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative from the current row. For example,\n",
      "     |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n",
      "     |      and \"5\" means the five off after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      :param start: boundary start, inclusive.\n",
      "     |                    The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |                    any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      "     |      :param end: boundary end, inclusive.\n",
      "     |                  The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |                  any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  rowsBetween(self, start, end)\n",
      "     |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n",
      "     |      \n",
      "     |      Both `start` and `end` are relative positions from the current row.\n",
      "     |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n",
      "     |      the current row, and \"5\" means the fifth row after the current row.\n",
      "     |      \n",
      "     |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      "     |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      "     |      values directly.\n",
      "     |      \n",
      "     |      :param start: boundary start, inclusive.\n",
      "     |                    The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      "     |                    any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      "     |      :param end: boundary end, inclusive.\n",
      "     |                  The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      "     |                  any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Window', 'WindowSpec']\n",
      "\n",
      "FILE\n",
      "    /opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/window.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Functions        | API or Function      |\n",
    "| ------------- |:-------------:|\n",
    "| Aggregate Functions      | <ul><li>sum</li><li>avg</li><li>min</li><li>max</li></ul> |\n",
    "| Ranking Functions      | <ul><li>rank</li><li>dense_rank</li></ul><ul><li>percent_rank</li><li>row_number</li> <li>ntile</li></ul> |\n",
    "| Analytic Functions      | <ul><li>cume_dist</li><li>first</li><li>last</li><li>lead</li> <li>lag</li></ul> |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
