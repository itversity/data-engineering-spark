{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using first and last functions\n",
    "\n",
    "Let us understand the usage of first and last value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Windowing Functions'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us get highest paid employee and least paid employee with in each department for each employee using employees data set.\n",
    "* You can also use max to get max salary for each department, but you cannot get other attributes related to ma salary such as employee id, name etc. With first or last, you can get other details as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesPath = '/public/hr_db/employees'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = spark. \\\n",
    "    read. \\\n",
    "    format('csv'). \\\n",
    "    option('sep', '\\t'). \\\n",
    "    schema('''employee_id INT, \n",
    "              first_name STRING, \n",
    "              last_name STRING, \n",
    "              email STRING,\n",
    "              phone_number STRING, \n",
    "              hire_date STRING, \n",
    "              job_id STRING, \n",
    "              salary FLOAT,\n",
    "              commission_pct STRING,\n",
    "              manager_id STRING, \n",
    "              department_id STRING\n",
    "            '''). \\\n",
    "    load(employeesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+\n",
      "|employee_id|department_id| salary|\n",
      "+-----------+-------------+-------+\n",
      "|        178|         null| 7000.0|\n",
      "|        200|           10| 4400.0|\n",
      "|        202|           20| 6000.0|\n",
      "|        201|           20|13000.0|\n",
      "|        119|           30| 2500.0|\n",
      "|        118|           30| 2600.0|\n",
      "|        117|           30| 2800.0|\n",
      "|        116|           30| 2900.0|\n",
      "|        115|           30| 3100.0|\n",
      "|        114|           30|11000.0|\n",
      "|        203|           40| 6500.0|\n",
      "|        132|           50| 2100.0|\n",
      "|        136|           50| 2200.0|\n",
      "|        128|           50| 2200.0|\n",
      "|        127|           50| 2400.0|\n",
      "|        135|           50| 2400.0|\n",
      "|        140|           50| 2500.0|\n",
      "|        144|           50| 2500.0|\n",
      "|        191|           50| 2500.0|\n",
      "|        182|           50| 2500.0|\n",
      "+-----------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees. \\\n",
    "    select('employee_id', \n",
    "           col('department_id').cast('int').alias('department_id'), \n",
    "           'salary'\n",
    "          ). \\\n",
    "    orderBy('department_id', 'salary'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window. \\\n",
    "    partitionBy('department_id'). \\\n",
    "    orderBy(col('salary').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+--------------+-------------------+\n",
      "|employee_id|department_id| salary|highest_salary|highest_employee_id|\n",
      "+-----------+-------------+-------+--------------+-------------------+\n",
      "|        178|         null| 7000.0|        7000.0|                178|\n",
      "|        200|           10| 4400.0|        4400.0|                200|\n",
      "|        201|           20|13000.0|       13000.0|                201|\n",
      "|        202|           20| 6000.0|       13000.0|                201|\n",
      "|        114|           30|11000.0|       11000.0|                114|\n",
      "|        115|           30| 3100.0|       11000.0|                114|\n",
      "|        116|           30| 2900.0|       11000.0|                114|\n",
      "|        117|           30| 2800.0|       11000.0|                114|\n",
      "|        118|           30| 2600.0|       11000.0|                114|\n",
      "|        119|           30| 2500.0|       11000.0|                114|\n",
      "|        203|           40| 6500.0|        6500.0|                203|\n",
      "|        121|           50| 8200.0|        8200.0|                121|\n",
      "|        120|           50| 8000.0|        8200.0|                121|\n",
      "|        122|           50| 7900.0|        8200.0|                121|\n",
      "|        123|           50| 6500.0|        8200.0|                121|\n",
      "|        124|           50| 5800.0|        8200.0|                121|\n",
      "|        184|           50| 4200.0|        8200.0|                121|\n",
      "|        185|           50| 4100.0|        8200.0|                121|\n",
      "|        192|           50| 4000.0|        8200.0|                121|\n",
      "|        193|           50| 3900.0|        8200.0|                121|\n",
      "+-----------+-------------+-------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees. \\\n",
    "    select('employee_id', \n",
    "           col('department_id').cast('int').alias('department_id'), \n",
    "           'salary'\n",
    "          ). \\\n",
    "    withColumn(\"highest_salary\", first('salary').over(spec)). \\\n",
    "    withColumn(\"highest_employee_id\", first('employee_id').over(spec)). \\\n",
    "    orderBy(\"department_id\", col(\"salary\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The default functionality of last function is to use the rows between unbounded preceding to current row. We need to change the rows between to unbounded preceding to unbounded following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window. \\\n",
    "    partitionBy('department_id'). \\\n",
    "    orderBy(col('salary').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+--------------+-------------------+\n",
      "|employee_id|department_id| salary|highest_salary|highest_employee_id|\n",
      "+-----------+-------------+-------+--------------+-------------------+\n",
      "|        178|         null| 7000.0|        7000.0|                178|\n",
      "|        200|           10| 4400.0|        4400.0|                200|\n",
      "|        201|           20|13000.0|       13000.0|                201|\n",
      "|        202|           20| 6000.0|        6000.0|                202|\n",
      "|        114|           30|11000.0|       11000.0|                114|\n",
      "|        115|           30| 3100.0|        3100.0|                115|\n",
      "|        116|           30| 2900.0|        2900.0|                116|\n",
      "|        117|           30| 2800.0|        2800.0|                117|\n",
      "|        118|           30| 2600.0|        2600.0|                118|\n",
      "|        119|           30| 2500.0|        2500.0|                119|\n",
      "|        203|           40| 6500.0|        6500.0|                203|\n",
      "|        121|           50| 8200.0|        8200.0|                121|\n",
      "|        120|           50| 8000.0|        8000.0|                120|\n",
      "|        122|           50| 7900.0|        7900.0|                122|\n",
      "|        123|           50| 6500.0|        6500.0|                123|\n",
      "|        124|           50| 5800.0|        5800.0|                124|\n",
      "|        184|           50| 4200.0|        4200.0|                184|\n",
      "|        185|           50| 4100.0|        4100.0|                185|\n",
      "|        192|           50| 4000.0|        4000.0|                192|\n",
      "|        193|           50| 3900.0|        3900.0|                193|\n",
      "+-----------+-------------+-------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees. \\\n",
    "    select('employee_id', \n",
    "           col('department_id').cast('int').alias('department_id'), \n",
    "           'salary'\n",
    "          ). \\\n",
    "    withColumn(\"highest_salary\", last('salary').over(spec)). \\\n",
    "    withColumn(\"highest_employee_id\", last('employee_id').over(spec)). \\\n",
    "    orderBy(\"department_id\", col(\"salary\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on WindowSpec in module pyspark.sql.window object:\n",
      "\n",
      "class WindowSpec(builtins.object)\n",
      " |  A window specification that defines the partitioning, ordering,\n",
      " |  and frame boundaries.\n",
      " |  \n",
      " |  Use the static methods in :class:`Window` to create a :class:`WindowSpec`.\n",
      " |  \n",
      " |  .. note:: Experimental\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, jspec)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  orderBy(self, *cols)\n",
      " |      Defines the ordering columns in a :class:`WindowSpec`.\n",
      " |      \n",
      " |      :param cols: names of columns or expressions\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  partitionBy(self, *cols)\n",
      " |      Defines the partitioning columns in a :class:`WindowSpec`.\n",
      " |      \n",
      " |      :param cols: names of columns or expressions\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rangeBetween(self, start, end)\n",
      " |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n",
      " |      \n",
      " |      Both `start` and `end` are relative from the current row. For example,\n",
      " |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n",
      " |      and \"5\" means the five off after the current row.\n",
      " |      \n",
      " |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      " |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      " |      values directly.\n",
      " |      \n",
      " |      :param start: boundary start, inclusive.\n",
      " |                    The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      " |                    any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      " |      :param end: boundary end, inclusive.\n",
      " |                  The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      " |                  any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rowsBetween(self, start, end)\n",
      " |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n",
      " |      \n",
      " |      Both `start` and `end` are relative positions from the current row.\n",
      " |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n",
      " |      the current row, and \"5\" means the fifth row after the current row.\n",
      " |      \n",
      " |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      " |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      " |      values directly.\n",
      " |      \n",
      " |      :param start: boundary start, inclusive.\n",
      " |                    The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      " |                    any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      " |      :param end: boundary end, inclusive.\n",
      " |                  The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      " |                  any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window. \\\n",
    "    partitionBy('department_id'). \\\n",
    "    orderBy(col('salary')). \\\n",
    "    rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+--------------+-------------------+\n",
      "|employee_id|department_id| salary|highest_salary|highest_employee_id|\n",
      "+-----------+-------------+-------+--------------+-------------------+\n",
      "|        178|         null| 7000.0|        7000.0|                178|\n",
      "|        200|           10| 4400.0|        4400.0|                200|\n",
      "|        201|           20|13000.0|       13000.0|                201|\n",
      "|        202|           20| 6000.0|        6000.0|                202|\n",
      "|        114|           30|11000.0|       11000.0|                114|\n",
      "|        115|           30| 3100.0|        3100.0|                115|\n",
      "|        116|           30| 2900.0|        2900.0|                116|\n",
      "|        117|           30| 2800.0|        2800.0|                117|\n",
      "|        118|           30| 2600.0|        2600.0|                118|\n",
      "|        119|           30| 2500.0|        2500.0|                119|\n",
      "|        203|           40| 6500.0|        6500.0|                203|\n",
      "|        121|           50| 8200.0|        8200.0|                121|\n",
      "|        120|           50| 8000.0|        8000.0|                120|\n",
      "|        122|           50| 7900.0|        7900.0|                122|\n",
      "|        123|           50| 6500.0|        6500.0|                123|\n",
      "|        124|           50| 5800.0|        5800.0|                124|\n",
      "|        184|           50| 4200.0|        4200.0|                184|\n",
      "|        185|           50| 4100.0|        4100.0|                185|\n",
      "|        192|           50| 4000.0|        4000.0|                192|\n",
      "|        193|           50| 3900.0|        3900.0|                193|\n",
      "+-----------+-------------+-------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees. \\\n",
    "    select('employee_id', \n",
    "           col('department_id').cast('int').alias('department_id'), \n",
    "           'salary'\n",
    "          ). \\\n",
    "    withColumn(\"highest_salary\", last('salary').over(spec)). \\\n",
    "    withColumn(\"highest_employee_id\", last('employee_id').over(spec)). \\\n",
    "    orderBy(\"department_id\", col(\"salary\").desc()). \\\n",
    "    show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
