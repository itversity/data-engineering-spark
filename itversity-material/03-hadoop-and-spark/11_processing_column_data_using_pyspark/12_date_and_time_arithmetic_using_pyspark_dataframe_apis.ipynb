{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time Arithmetic\n",
    "Let us perform Date and Time Arithmetic using relevant functions over Spark Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jw9ZR_Iz2dU?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jw9ZR_Iz2dU?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding days to a date or timestamp - `date_add`\n",
    "* Subtracting days from a date or timestamp - `date_sub`\n",
    "* Getting difference between 2 dates or timestamps - `datediff`\n",
    "* Getting the number of months between 2 dates or timestamps - `months_between`\n",
    "* Adding months to a date or timestamp - `add_months`\n",
    "* Getting next day from a given date - `next_day`\n",
    "* All the functions are self explanatory. We can apply these on standard date or timestamp. All the functions return date even when applied on timestamp field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Processing Column Data'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "\n",
    "Let us perform some tasks related to date arithmetic.\n",
    "* Get help on each and every function first and understand what all arguments need to be passed.\n",
    "* Create a Dataframe by name datetimesDF with columns date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add 10 days to both date and time values.\n",
    "* Subtract 10 days from both date and time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_add in module pyspark.sql.functions:\n",
      "\n",
      "date_add(start, days)\n",
      "    Returns the date that is `days` days after `start`\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "    [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_sub in module pyspark.sql.functions:\n",
      "\n",
      "date_sub(start, days)\n",
      "    Returns the date that is `days` days before `start`\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "    [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|      date|                time|date_add_date|date_add_time|date_sub_date|date_sub_time|\n",
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|   2014-03-10|   2014-03-10|   2014-02-18|   2014-02-18|\n",
      "|2016-02-29|2016-02-29 08:08:...|   2016-03-10|   2016-03-10|   2016-02-19|   2016-02-19|\n",
      "|2017-10-31|2017-12-31 11:59:...|   2017-11-10|   2018-01-10|   2017-10-21|   2017-12-21|\n",
      "|2019-11-30|2019-08-31 00:00:...|   2019-12-10|   2019-09-10|   2019-11-20|   2019-08-21|\n",
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_add_date\", date_add(\"date\", 10)). \\\n",
    "    withColumn(\"date_add_time\", date_add(\"time\", 10)). \\\n",
    "    withColumn(\"date_sub_date\", date_sub(\"date\", 10)). \\\n",
    "    withColumn(\"date_sub_time\", date_sub(\"time\", 10)). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the difference between current_date and date values as well as current_timestamp and time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+-------------+\n",
      "|      date|                time|datediff_date|datediff_time|\n",
      "+----------+--------------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|         2559|         2559|\n",
      "|2016-02-29|2016-02-29 08:08:...|         1828|         1828|\n",
      "|2017-10-31|2017-12-31 11:59:...|         1218|         1157|\n",
      "|2019-11-30|2019-08-31 00:00:...|          458|          549|\n",
      "+----------+--------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"datediff_date\", datediff(current_date(), \"date\")). \\\n",
    "    withColumn(\"datediff_time\", datediff(current_timestamp(), \"time\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the number of months between current_date and date values as well as current_timestamp and time values.\n",
    "* Add 3 months to both date values as well as time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import months_between, add_months, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|84.16              |84.17              |2014-05-31     |2014-05-31     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|60.13              |60.14              |2016-05-31     |2016-05-31     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|40.06              |38.07              |2018-01-31     |2018-03-31     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|15.1               |18.08              |2020-02-29     |2019-11-30     |\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"months_between_date\", round(months_between(current_date(), \"date\"), 2)). \\\n",
    "    withColumn(\"months_between_time\", round(months_between(current_timestamp(), \"time\"), 2)). \\\n",
    "    withColumn(\"add_months_date\", add_months(\"date\", 3)). \\\n",
    "    withColumn(\"add_months_time\", add_months(\"time\", 3)). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
