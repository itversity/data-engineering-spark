{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Sorting Data Frames\n",
    "\n",
    "Let us understand how to sort the data in a Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V7uINKwev_k?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V7uINKwev_k?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use `orderBy` or `sort` to sort the data.\n",
    "* We can perform composite sorting by passing multiple columns or expressions.\n",
    "* By default data is sorted in ascending order, we can change it to descending by applying `desc()` function on the column or expression.\n",
    "* If the sort column contain null values those will come first. We can change the position of nulls to last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Basic Transformations'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = \"/public/airtraffic_all/airtraffic-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark. \\\n",
    "    read. \\\n",
    "    parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get daily count of cancelled flights where data is sorted in ascending order by count of cancelled flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lpad, lit, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080120|        247|\n",
      "|  20080130|        694|\n",
      "|  20080115|        299|\n",
      "|  20080118|        230|\n",
      "|  20080122|        788|\n",
      "|  20080104|        769|\n",
      "|  20080125|        526|\n",
      "|  20080102|        511|\n",
      "|  20080105|        456|\n",
      "|  20080111|        524|\n",
      "|  20080109|        377|\n",
      "|  20080127|        638|\n",
      "|  20080101|        552|\n",
      "|  20080128|        654|\n",
      "|  20080119|        876|\n",
      "|  20080106|        683|\n",
      "|  20080123|        530|\n",
      "|  20080117|        872|\n",
      "|  20080116|        532|\n",
      "|  20080112|        226|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"), 2, \"0\"),\n",
    "            lpad(col(\"DayOfMonth\"), 2, \"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080112|        226|\n",
      "|  20080118|        230|\n",
      "|  20080120|        247|\n",
      "|  20080115|        299|\n",
      "|  20080124|        322|\n",
      "|  20080110|        341|\n",
      "|  20080113|        359|\n",
      "|  20080109|        377|\n",
      "|  20080126|        416|\n",
      "|  20080105|        456|\n",
      "|  20080108|        463|\n",
      "|  20080103|        475|\n",
      "|  20080121|        475|\n",
      "|  20080102|        511|\n",
      "|  20080111|        524|\n",
      "|  20080125|        526|\n",
      "|  20080123|        530|\n",
      "|  20080116|        532|\n",
      "|  20080101|        552|\n",
      "|  20080107|        579|\n",
      "|  20080127|        638|\n",
      "|  20080128|        654|\n",
      "|  20080106|        683|\n",
      "|  20080130|        694|\n",
      "|  20080104|        769|\n",
      "|  20080122|        788|\n",
      "|  20080117|        872|\n",
      "|  20080119|        876|\n",
      "|  20080129|        889|\n",
      "|  20080114|        909|\n",
      "|  20080131|       1081|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"), 2, \"0\"),\n",
    "            lpad(col(\"DayOfMonth\"), 2, \"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy('FlightCount'). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080112|        226|\n",
      "|  20080118|        230|\n",
      "|  20080120|        247|\n",
      "|  20080115|        299|\n",
      "|  20080124|        322|\n",
      "|  20080110|        341|\n",
      "|  20080113|        359|\n",
      "|  20080109|        377|\n",
      "|  20080126|        416|\n",
      "|  20080105|        456|\n",
      "|  20080108|        463|\n",
      "|  20080103|        475|\n",
      "|  20080121|        475|\n",
      "|  20080102|        511|\n",
      "|  20080111|        524|\n",
      "|  20080125|        526|\n",
      "|  20080123|        530|\n",
      "|  20080116|        532|\n",
      "|  20080101|        552|\n",
      "|  20080107|        579|\n",
      "|  20080127|        638|\n",
      "|  20080128|        654|\n",
      "|  20080106|        683|\n",
      "|  20080130|        694|\n",
      "|  20080104|        769|\n",
      "|  20080122|        788|\n",
      "|  20080117|        872|\n",
      "|  20080119|        876|\n",
      "|  20080129|        889|\n",
      "|  20080114|        909|\n",
      "|  20080131|       1081|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"), 2, \"0\"),\n",
    "            lpad(col(\"DayOfMonth\"), 2, \"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy(col('FlightCount').asc()). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get daily count of cancelled flights where data is sorted in descending order by count of cancelled flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080131|       1081|\n",
      "|  20080114|        909|\n",
      "|  20080129|        889|\n",
      "|  20080119|        876|\n",
      "|  20080117|        872|\n",
      "|  20080122|        788|\n",
      "|  20080104|        769|\n",
      "|  20080130|        694|\n",
      "|  20080106|        683|\n",
      "|  20080128|        654|\n",
      "|  20080127|        638|\n",
      "|  20080107|        579|\n",
      "|  20080101|        552|\n",
      "|  20080116|        532|\n",
      "|  20080123|        530|\n",
      "|  20080125|        526|\n",
      "|  20080111|        524|\n",
      "|  20080102|        511|\n",
      "|  20080121|        475|\n",
      "|  20080103|        475|\n",
      "|  20080108|        463|\n",
      "|  20080105|        456|\n",
      "|  20080126|        416|\n",
      "|  20080109|        377|\n",
      "|  20080113|        359|\n",
      "|  20080110|        341|\n",
      "|  20080124|        322|\n",
      "|  20080115|        299|\n",
      "|  20080120|        247|\n",
      "|  20080118|        230|\n",
      "|  20080112|        226|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"), 2, \"0\"),\n",
    "            lpad(col(\"DayOfMonth\"), 2, \"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy(col('FlightCount').desc()). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Project Year, Month, DayOfMonth, CRSDepTime and Origin for this task.\n",
    "* Sort the data based up on year, month, day of month and then using scheduled time. Data should be sorted in ascending order by year, month and day of month then in descending order by scheduled time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+------+\n",
      "|Year|Month|DayOfMonth|CRSDepTime|Origin|\n",
      "+----+-----+----------+----------+------+\n",
      "|2008|    1|        16|      1735|   BGR|\n",
      "|2008|    1|        17|      1701|   SYR|\n",
      "|2008|    1|        17|      1225|   SAV|\n",
      "|2008|    1|        17|      1530|   CVG|\n",
      "|2008|    1|        17|      1205|   STL|\n",
      "|2008|    1|        18|      1150|   STL|\n",
      "|2008|    1|        18|      1009|   MCI|\n",
      "|2008|    1|        19|       835|   TUL|\n",
      "|2008|    1|        20|      1935|   JFK|\n",
      "|2008|    1|        20|       830|   RDU|\n",
      "|2008|    1|        21|      1640|   CVG|\n",
      "|2008|    1|        21|      1204|   MSY|\n",
      "|2008|    1|        21|      1935|   JFK|\n",
      "|2008|    1|        21|      1830|   DCA|\n",
      "|2008|    1|        21|       700|   HSV|\n",
      "|2008|    1|        22|      1910|   ORD|\n",
      "|2008|    1|        22|      1320|   CVG|\n",
      "|2008|    1|        23|       908|   LGA|\n",
      "|2008|    1|        23|      1252|   CLT|\n",
      "|2008|    1|        23|       635|   GSP|\n",
      "+----+-----+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year', 'Month', 'DayOfMonth', 'CRSDepTime', 'Origin'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+------+\n",
      "|Year|Month|DayOfMonth|CRSDepTime|Origin|\n",
      "+----+-----+----------+----------+------+\n",
      "|2008|    1|         1|        10|   LAX|\n",
      "|2008|    1|         1|        15|   SMF|\n",
      "|2008|    1|         1|        25|   SMF|\n",
      "|2008|    1|         1|        25|   PHX|\n",
      "|2008|    1|         1|        30|   ANC|\n",
      "|2008|    1|         1|        30|   LAX|\n",
      "|2008|    1|         1|        30|   LAS|\n",
      "|2008|    1|         1|        30|   ONT|\n",
      "|2008|    1|         1|        35|   MCO|\n",
      "|2008|    1|         1|        35|   SFO|\n",
      "|2008|    1|         1|        40|   LAX|\n",
      "|2008|    1|         1|        40|   LAS|\n",
      "|2008|    1|         1|        40|   LAX|\n",
      "|2008|    1|         1|        40|   SEA|\n",
      "|2008|    1|         1|        40|   SFO|\n",
      "|2008|    1|         1|        40|   SEA|\n",
      "|2008|    1|         1|        45|   PHX|\n",
      "|2008|    1|         1|        45|   LAS|\n",
      "|2008|    1|         1|        50|   ANC|\n",
      "|2008|    1|         1|        53|   PDX|\n",
      "+----+-----+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year', 'Month', 'DayOfMonth', 'CRSDepTime', 'Origin'). \\\n",
    "    orderBy('Year', 'Month', 'DayOfMonth', 'CRSDepTime'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+------+\n",
      "|Year|Month|DayOfMonth|CRSDepTime|Origin|\n",
      "+----+-----+----------+----------+------+\n",
      "|2008|    1|         1|      2359|   PHX|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   PHX|\n",
      "|2008|    1|         1|      2359|   SLC|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   SLC|\n",
      "|2008|    1|         1|      2359|   SEA|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   TUS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2358|   LAS|\n",
      "|2008|    1|         1|      2358|   LAS|\n",
      "|2008|    1|         1|      2356|   LAS|\n",
      "+----+-----+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year', 'Month', 'DayOfMonth', 'CRSDepTime', 'Origin'). \\\n",
    "    orderBy('Year', 'Month', 'DayOfMonth', col('CRSDepTime').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create employees Data Frame and get employees data in ascending order by nationality. However, data related to United States should come at top always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 2,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
       "If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
       "\n",
       ":param condition: a boolean :class:`Column` expression.\n",
       ":param value: a literal value, or a :class:`Column` expression.\n",
       "\n",
       ">>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
       "[Row(age=3), Row(age=4)]\n",
       "\n",
       ">>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
       "[Row(age=3), Row(age=None)]\n",
       "\n",
       ".. versionadded:: 1.4\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "when?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|sort_column|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|          0|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|          1|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|          1|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|          1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('sort_column', when(upper(col('nationality')) == 'UNITED STATES', 0).otherwise(1)). \\\n",
    "    orderBy('sort_column', 'nationality'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|sort_column|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|          0|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|          1|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|          1|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|          1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn(\n",
    "        'sort_column', \n",
    "        expr(\"\"\"CASE WHEN upper(nationality) = 'UNITED STATES' THEN 0 else 1 END\"\"\")\n",
    "    ). \\\n",
    "    orderBy('sort_column', 'nationality'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sort the data in employeesDF using bonus. Data should be sorted numerically and null and empty values should come at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- bonus: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy('bonus'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(col('bonus').cast('int')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = col('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method _ in module pyspark.sql.column:\n",
      "\n",
      "_() method of pyspark.sql.column.Column instance\n",
      "    Returns a sort expression based on ascending order of the column, and null values\n",
      "    appear after non-null values.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "    >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      "    [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      "    \n",
      "    .. versionadded:: 2.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(c.asc_nulls_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(employeesDF.bonus.cast('int').asc_nulls_last()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
