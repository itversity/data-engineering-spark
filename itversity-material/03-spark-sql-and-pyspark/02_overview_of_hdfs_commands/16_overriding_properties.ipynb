{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding Properties\n",
    "\n",
    "Let us understand how we can override the properties while running `hdfs dfs` or `hadoop fs` commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can change any property which is not defined as final in **core-site.xml** or **hdfs-site.xml**.\n",
    "* We can change `blocksize` as well as `replication` while copying the files. We can also change them after copying the files as well.\n",
    "* We can either pass individual properties using `-D` or bunch of properties by passing xml similar to **core-site.xml** or **hdfs-site.xml** as part of `--conf`.\n",
    "* Let's copy a file **/data/crime/csv/rows.csv** with default values. The file is splitted into 12 blocks with 2 copies each (as our default blocksize is 128 MB and replication factor is 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - itversity students          0 2021-01-28 17:03 /user/itversity/crime/csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/itversity/crime\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R -skipTrash /user/${USER}/crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/${USER}/crime/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.5G\n",
      "-rw-r--r-- 1 root root 1.5G Aug  8  2017 rows.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "ls -lhtr /data/crime/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -put /data/crime/csv/rows.csv /user/${USER}/crime/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %r /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %o /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1505540526\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %b /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/itversity/crime/csv/rows.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R -skipTrash /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -Ddfs.blocksize=64M -Ddfs.replication=3 -put /data/crime/csv/rows.csv /user/${USER}/crime/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %r /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67108864\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %o /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1505540526\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %b /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 196\n",
      "-rw-r--r-- 1 root   root    2250 May 11  2018 yarn-env.cmd\n",
      "-rw-r--r-- 1 mapred hadoop  2697 May 11  2018 ssl-server.xml.example\n",
      "-rw-r--r-- 1 mapred hadoop  2316 May 11  2018 ssl-client.xml.example\n",
      "-rw-r--r-- 1 root   root     758 May 11  2018 mapred-site.xml.template\n",
      "-rw-r--r-- 1 root   root    4113 May 11  2018 mapred-queues.xml.template\n",
      "-rw-r--r-- 1 root   root     951 May 11  2018 mapred-env.cmd\n",
      "-rw-r--r-- 1 root   root    5511 May 11  2018 kms-site.xml\n",
      "-rw-r--r-- 1 root   root    1631 May 11  2018 kms-log4j.properties\n",
      "-rw-r--r-- 1 root   root    1527 May 11  2018 kms-env.sh\n",
      "-rw-r--r-- 1 root   root    3518 May 11  2018 kms-acls.xml\n",
      "-rw-r--r-- 1 root   root    2490 May 11  2018 hadoop-metrics.properties\n",
      "-rw-r--r-- 1 root   root    3979 May 11  2018 hadoop-env.cmd\n",
      "-rw-r--r-- 1 hdfs   hadoop  1335 May 11  2018 configuration.xsl\n",
      "-rw-r--r-- 1 hdfs   hadoop  1308 Mar  3  2020 hadoop-policy.xml\n",
      "-rw-r--r-- 1 hdfs   hadoop   884 Mar  3  2020 ssl-client.xml\n",
      "drwxr-xr-x 2 root   hadoop  4096 Mar  3  2020 secure\n",
      "-rw-r--r-- 1 hdfs   hadoop  1000 Mar  3  2020 ssl-server.xml\n",
      "-rw-r--r-- 1 hdfs   hadoop  6531 Mar  3  2020 hdfs-site.xml\n",
      "-rw-r--r-- 1 hdfs   root      96 Mar  3  2020 slaves\n",
      "-rw-r--r-- 1 mapred hadoop  6984 Mar  3  2020 mapred-site.xml\n",
      "-rw-r--r-- 1 hdfs   hadoop  2135 Mar  3  2020 capacity-scheduler.xml\n",
      "-rwxr-xr-x 1 yarn   hadoop  5359 Mar  3  2020 yarn-env.sh\n",
      "-rw-r--r-- 1 root   hadoop  1019 Mar  3  2020 container-executor.cfg\n",
      "-rwxr-xr-x 1 hdfs   root     818 Mar  3  2020 mapred-env.sh\n",
      "-rw-r--r-- 1 hdfs   root     945 Mar  3  2020 taskcontroller.cfg\n",
      "-rw-r--r-- 1 hdfs   root    1020 Mar  3  2020 commons-logging.properties\n",
      "-rw-r--r-- 1 hdfs   root    1602 Mar  3  2020 health_check\n",
      "-rw-r--r-- 1 hdfs   hadoop  2263 Mar  3  2020 hadoop-metrics2.properties\n",
      "-rwxr-xr-x 1 root   root    4221 Mar  3  2020 task-log4j.properties\n",
      "-rw-r--r-- 1 hdfs   hadoop   319 Mar  3  2020 topology_mappings.data\n",
      "-rwxr-xr-x 1 root   root    2358 Mar  3  2020 topology_script.py\n",
      "-rw-r--r-- 1 hdfs   hadoop 10495 May  6  2020 log4j.properties\n",
      "-rw-r--r-- 1 yarn   hadoop 18438 Dec 14 02:11 yarn-site.xml\n",
      "-rw-r--r-- 1 hdfs   hadoop  5705 Dec 14 02:40 hadoop-env.sh\n",
      "-rw-r--r-- 1 hdfs   hadoop  4990 Jan  8 11:20 core-site.xml\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "ls -ltr /etc/hadoop/conf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  <configuration>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.block.access.token.enable</name>\n",
      "      <value>true</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.blockreport.initialDelay</name>\n",
      "      <value>120</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.blocksize</name>\n",
      "      <value>134217728</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.client.read.shortcircuit</name>\n",
      "      <value>true</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.client.read.shortcircuit.streams.cache.size</name>\n",
      "      <value>4096</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.client.retry.policy.enabled</name>\n",
      "      <value>false</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.cluster.administrators</name>\n",
      "      <value> hdfs</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.content-summary.limit</name>\n",
      "      <value>5000</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.datanode.address</name>\n",
      "      <value>0.0.0.0:50010</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.datanode.balance.bandwidthPerSec</name>\n",
      "      <value>6250000</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.datanode.data.dir</name>\n",
      "      <value>/hdp01/hadoop/hdfs/data,/hdp02/hadoop/hdfs/data</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.datanode.data.dir.perm</name>\n",
      "      <value>750</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.datanode.du.reserved</name>\n",
      "      <value>2563350016</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.datanode.failed.volumes.tolerated</name>\n",
      "      <value>0</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.datanode.http.address</name>\n",
      "      <value>0.0.0.0:50075</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.datanode.https.address</name>\n",
      "      <value>0.0.0.0:50475</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.datanode.ipc.address</name>\n",
      "      <value>0.0.0.0:8010</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.datanode.max.transfer.threads</name>\n",
      "      <value>4096</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.domain.socket.path</name>\n",
      "      <value>/var/lib/hadoop-hdfs/dn_socket</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.encrypt.data.transfer.cipher.suites</name>\n",
      "      <value>AES/CTR/NoPadding</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.heartbeat.interval</name>\n",
      "      <value>3</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.hosts.exclude</name>\n",
      "      <value>/etc/hadoop/conf/dfs.exclude</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.http.policy</name>\n",
      "      <value>HTTP_ONLY</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.https.port</name>\n",
      "      <value>50470</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.journalnode.edits.dir</name>\n",
      "      <value>/hadoop/hdfs/journalnode</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.journalnode.http-address</name>\n",
      "      <value>0.0.0.0:8480</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.journalnode.https-address</name>\n",
      "      <value>0.0.0.0:8481</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.accesstime.precision</name>\n",
      "      <value>3600000</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.audit.log.async</name>\n",
      "      <value>true</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.avoid.read.stale.datanode</name>\n",
      "      <value>true</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.avoid.write.stale.datanode</name>\n",
      "      <value>true</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.checkpoint.dir</name>\n",
      "      <value>/var/hadoop/hdfs/namesecondary</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.checkpoint.edits.dir</name>\n",
      "      <value>${dfs.namenode.checkpoint.dir}</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.checkpoint.period</name>\n",
      "      <value>21600</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.checkpoint.txns</name>\n",
      "      <value>1000000</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.fslock.fair</name>\n",
      "      <value>false</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.handler.count</name>\n",
      "      <value>200</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.http-address</name>\n",
      "      <value>172.16.1.101:50070</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.https-address</name>\n",
      "      <value>nn01.itversity.com:50470</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.name.dir</name>\n",
      "      <value>/hdp01/hadoop/hdfs/namenode,/hdp02/hadoop/hdfs/namenode</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.name.dir.restore</name>\n",
      "      <value>true</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.rpc-address</name>\n",
      "      <value>nn01.itversity.com:8020</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.safemode.threshold-pct</name>\n",
      "      <value>1</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.secondary.http-address</name>\n",
      "      <value>nn02.itversity.com:50090</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.stale.datanode.interval</name>\n",
      "      <value>30000</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.startup.delay.block.deletion.sec</name>\n",
      "      <value>3600</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.namenode.write.stale.datanode.ratio</name>\n",
      "      <value>1.0f</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.permissions.enabled</name>\n",
      "      <value>true</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.permissions.superusergroup</name>\n",
      "      <value>hdfs</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.replication</name>\n",
      "      <value>2</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.replication.max</name>\n",
      "      <value>50</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.support.append</name>\n",
      "      <value>true</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>dfs.webhdfs.enabled</name>\n",
      "      <value>true</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>fs.permissions.umask-mode</name>\n",
      "      <value>022</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>nfs.exports.allowed.hosts</name>\n",
      "      <value>* rw</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "      <name>nfs.file.dump.dir</name>\n",
      "      <value>/tmp/.hdfs-nfs</value>\n",
      "    </property>\n",
      "    \n",
      "  </configuration>"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat /etc/hadoop/conf/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
